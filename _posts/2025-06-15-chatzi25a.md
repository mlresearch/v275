---
title: Counterfactual Token Generation in Large Language Models
booktitle: Proceedings of the Fourth Conference on Causal Learning and Reasoning
year: '2025'
volume: '275'
series: Proceedings of Machine Learning Research
month: 5
publisher: PMLR
openreview: YYztiQ2Vke
abstract: '*"Sure, I am happy to generate a story for you: Captain Lyra stood at the
  helm of her trusty ship, the Maelstrom’s Fury, gazing out at the endless sea. [...]
  Lyra’s eyes welled up with tears as she realized the bitter truth - she had sacrificed
  everything for fleeting riches, and lost the love of her crew, her family, and herself."*
  Although this story, generated by a large language model, is captivating, one may
  wonder—how would the story have unfolded if the model had chosen "Captain Maeve"
  as the protagonist instead? We cannot know. State-of-the-art large language models
  are stateless—they maintain no internal memory or state. Given a prompt, they generate
  a sequence of tokens as an output using an autoregressive process. As a consequence,
  they cannot reason about counterfactual alternatives to tokens they have generated
  in the past. In this work, our goal is to enhance them with this functionality.
  To this end, we develop a causal model of token generation that builds upon the
  Gumbel-Max structural causal model. Our model allows any large language model to
  perform counterfactual token generation at almost no cost in comparison with vanilla
  token generation, it is embarrassingly simple to implement, and it does not require
  any fine-tuning nor prompt engineering. We implement our model on Llama 3 8B-Instruct
  and Ministral-8B-Instruct, and conduct a qualitative and a quantitative analysis
  of counterfactually generated text. We conclude with a demonstrative application
  of counterfactual token generation for bias detection, unveiling interesting insights
  about the model of the world constructed by large language models.'
layout: inproceedings
issn: 2640-3498
id: chatzi25a
tex_title: Counterfactual Token Generation in Large Language Models
firstpage: 1291
lastpage: 1315
page: 1291-1315
order: 1291
cycles: false
bibtex_editor: Huang, Biwei and Drton, Mathias
editor:
- given: Biwei
  family: Huang
- given: Mathias
  family: Drton
bibtex_author: Chatzi, Ivi and Corvelo Benz, Nina L. and Straitouri, Eleni and Tsirtsis,
  Stratis and Gomez-Rodriguez, Manuel
author:
- given: Ivi
  family: Chatzi
- given: Nina L.
  family: Corvelo Benz
- given: Eleni
  family: Straitouri
- given: Stratis
  family: Tsirtsis
- given: Manuel
  family: Gomez-Rodriguez
date: 2025-06-15
address:
container-title: Proceedings of the Fourth Conference on Causal Learning and Reasoning
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v275/main/assets/chatzi25a/chatzi25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
