@Proceedings{CLeaR2025,
	title = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	name = {Causal Learning and Reasoning},
	shortname = {CLeaR},
	conference_number = {4},
	year = {2025},
	start = {2025-05-07},
	end = {2025-05-09},
	published = {2025-06-15},
	address = {Lausanne, Switzerland},
	conference_url = {https://www.cclear.cc/2025},
	editor = {Huang, Biwei and Drton, Mathias},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	volume = {275}
}

@InProceedings{pmlr-v275-cakiqi25a,
	title = {Algorithmic syntactic causal identification},
	author = {Cakiqi, Dhurim and Little, Max A},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1--14},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {Hdb7dLkXuu},
	abstract = {Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions
where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the
mathematical language of classical probability theory on CBNs. However, there are many
causal settings where probability theory and hence current causal identification techniques
are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We
show that this restriction can be lifted by replacing the use of classical probability theory
with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn
between the general syntax of causal models and any specific semantic implementation of
that causal model. This allows a purely syntactic algorithmic description of general causal
identification by a translation of recent formulations of the general ID algorithm through
fixing. Our description is given entirely in terms of the non-parametric ADMG structure
specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified
monoidal category in which the desired, purely syntactic interventional causal model, is
obtained. We use this idea to derive purely syntactic analogues of classical back-door and
front-door causal adjustment, and illustrate an application to a more complex causal model.}
}

@InProceedings{pmlr-v275-assaad25a,
	title = {Causal reasoning in difference  graphs},
	author = {Assaad, Charles K.},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {15--30},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {yYGdFo4kKb},
	abstract = {Understanding causal mechanisms across different populations is essential for designing effective public health interventions. Recently, difference graphs have been introduced as a tool to visually represent causal variations between two distinct populations. While there has been progress in inferring these graphs from data through causal discovery methods, there remains a gap in systematically leveraging their potential to enhance causal reasoning. This paper addresses that gap by establishing conditions for identifying causal changes and effects using difference graphs. It specifically focuses on identifying total causal changes and total effects in a nonparametric setting, as well as direct causal changes and direct effects in a linear setting. In doing so, it provides a novel approach to  causal reasoning that holds potential for various public health applications.}
}

@InProceedings{pmlr-v275-konobeev25a,
	title = {Causal Bandits without Graph Learning},
	author = {Konobeev, Mikhail and Etesami, Jalal and Kiyavash, Negar},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {31--63},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {v32DGAfc9n},
	abstract = {We study the causal bandit problem when the causal graph is unknown and develop
an efficient algorithm for finding the parent node of the reward node using
atomic interventions. We derive the exact equation for the expected number of
interventions performed by the algorithm and show that under certain graphical
conditions it could perform either logarithmically fast or, under more general
assumptions, slower but still sublinearly in the number of variables.
We formally show that our algorithm is optimal as it meets the universal lower
bound we establish for any algorithm that performs atomic interventions.
Finally, we extend our algorithm to the case when the reward node has multiple
parents.  Using this algorithm together with a standard algorithm from bandit
literature leads to improved regret bounds.}
}

@InProceedings{pmlr-v275-manten25a,
	title = {An Asymmetric Independence Model for Causal Discovery on Path Spaces},
	author = {Manten, Georg and Casolo, Cecilia and Mogensen, S\o{}ren Wengel and Kilbertus, Niki},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {64--89},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {xQuDYrWbud},
	abstract = {In this paper, we develop the theory linking directed mixed graphs (DMGs) with the graphical 'E-separation'-criterion to form asymmetric independence models that are closed under marginalization and which graphically describe the conditional independence relations among coordinate processes in stochastic differential equations (SDEs) when testing "which variables enter the governing equations of which other variables." Besides a global Markov property for cyclic SDEs, which naturally extends to latent, cyclic SDEs, we also characterize graphs, which encode the same set of independence relations and show that in the fully observed case, modelled by directed graphs, each class of graphs under this equivalence relation has a maximal element that is graphi- cally characterizable, analogous to the famous 'same skeleton and V-structure' result for Directed Acyclic Graphs (DAGs) and in addition is recoverable from data. Moreover, we conjecture that same holds true in the partially observed case and verify this empirically for graphs up to 4 nodes.}
}

@InProceedings{pmlr-v275-de-aguas25a,
	title = {The Probability of Tiered Benefit: Partial Identification with Robust and Stable Inference},
	author = {de Aguas, Johan and Krumscheid, Sebastian and Pensar, Johan and Biele, Guido},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {90--113},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {an73Bb58GV},
	abstract = {We define the Probability of Tiered Benefit in scenarios with a binary exposure  and an outcome that is either categorical with $K \geq 2$ ordered tiers or continuous partitioned by $K-1$ fixed thresholds into disjoint intervals. Similarly to other pure counterfactual queries, this parameter is not $g$-identifiable without additional assumptions. We demonstrate that strong monotonicity does not suffice for point identification when $K \geq 3$ and provide sharp bounds both with and without such constraint. Inference and uncertainty quantification for these bounds are challenging tasks due to potential nonregularity induced by ambiguities in the underlying individualized optimization problems. Such ambiguities can arise from immunities or null treatment effects in subpopulations with positive probability, affecting the lower bound estimate and hindering conservative inference. To address these issues, we extend the available Stabilized One-Step Correction (S1S) procedure by incorporating stratum-specific stabilizing matrices. Through simulations, we illustrate the benefits of this approach over existing alternatives. We apply our method to estimate bounds on the probabilities of tiered benefit and harm from pharmacological treatment for ADHD upon academic achievement, employing observational data from diagnosed Norwegian schoolchildren. Our findings indicate that while girls and children with low prior test performance could have moderate chances of both benefit and harm from treatment, a clear-cut recommendation remains uncertain across all strata.}
}

@InProceedings{pmlr-v275-pislar25a,
	title = {Combining Causal Models for More Accurate Abstractions of Neural Networks},
	author = {P\^{i}slar, Theodora-Mara and Magliacane, Sara and Geiger, Atticus},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {114--138},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {mVftlEi1CD},
	abstract = {Mechanistic interpretability aims to reverse engineer neural networks by uncovering which high-level algorithms they implement. Causal abstraction provides a precise notion of when a network implements an algorithm, i.e., a causal model of the network contains low-level features that realize the high-level variables in a causal model of the algorithm (Geiger et al., 2024). A typical problem in practical settings is that the algorithm is not an entirely faithful abstraction of the network, i.e., it only partially captures true reasoning process of a model. We propose a solution where we combine different simple high-level models to produce a more faithful representation of the network. Through learning this combination, we can model neural networks as being in different computational states depending on the input provided, which we show is more accurate to GPT-2 small fine-tuned on two toy tasks. We observe a trade off between the strength of an interpretability hypothesis, which we define in terms of the number of inputs explained by the high-level models, and its faithfulness, which we define as the interchange intervention accuracy. Our method allows us to modulate between the two, providing the most accurate combination of models that describe the behavior of a neural network given a faithfulness level.}
}

@InProceedings{pmlr-v275-van-der-laan25a,
	title = {Stabilized Inverse Probability Weighting via Isotonic Calibration},
	author = {van der Laan, Lars and Lin, Ziming and Carone, Marco and Luedtke, Alex},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {139--173},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {h9VANDOAfm},
	abstract = {Inverse weighting with an estimated propensity score is widely used by estimation methods in causal inference to adjust for confounding bias. However, directly inverting propensity score estimates can lead to instability, bias, and excessive variability due to large inverse weights, especially when treatment overlap is limited. In this work, we propose a post-hoc calibration algorithm for inverse propensity weights that generates well-calibrated, stabilized weights from user-supplied, cross-fitted propensity score estimates. Our approach employs a variant of isotonic regression with a loss function specifically tailored to the inverse propensity weights. Through theoretical analysis and empirical studies, we demonstrate that isotonic calibration improves the performance of doubly robust estimators of the average treatment effect.}
}

@InProceedings{pmlr-v275-leiner25a,
	title = {Scalable Causal Structure Learning via Amortized Conditional Independence Testing},
	author = {Leiner, James and Manzo, Brian and Ramdas, Aaditya and Tansey, Wesley},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {174--200},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {Ai6d4mnoEF},
	abstract = {Controlling false positives (Type I errors) through statistical hypothesis testing is a foundation
of modern scientific data analysis. Existing causal structure discovery algorithms either do not
provide Type I error control or cannot scale to the size of modern scientific datasets. We consider
a variant of the causal discovery problem with two sets of nodes, where the only edges of interest
form a bipartite causal subgraph between the sets. We develop Scalable Causal Structure Learning
(SCSL), a method for causal structure discovery on bipartite subgraphs that provides Type I error
control. SCSL recasts the discovery problem as a simultaneous hypothesis testing problem and
uses discrete optimization over the set of possible confounders to obtain an upper bound on the
test statistic for each edge. Semi-synthetic simulations demonstrate that SCSL scales to handle
graphs with hundreds of nodes while maintaining error control and good power. We demonstrate the
practical applicability of the method by applying it to a cancer dataset to reveal connections between
somatic gene mutations and metastases to different tissues.}
}

@InProceedings{pmlr-v275-wendong25a,
	title = {Algorithmic causal structure emerging through compression},
	author = {Wendong, Liang and Buchholz, Simon and Sch\"{o}lkopf, Bernhard},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {201--242},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {YFpnQRsrEr},
	abstract = {We explore the relationship between causality, symmetry, and compression. We build on and generalize the known connection between learning and compression to a setting where causal models are not identifiable. We propose a framework where causality emerges as a  consequence of compressing data across multiple environments. We define algorithmic causality as an alternative definition of causality when traditional assumptions for causal identifiability do not hold. We demonstrate how algorithmic causal and symmetric structures can emerge from minimizing upper bounds on Kolmogorov complexity, without knowledge of intervention targets. We hypothesize that these insights may also provide a novel perspective on the emergence of causality in machine learning models, such as large language models, where causal relationships may not be explicitly identifiable.}
}

@InProceedings{pmlr-v275-fatemi25a,
	title = {Contagion Effect Estimation Using Proximal Embeddings},
	author = {Fatemi, Zahra and Zheleva, Elena},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {243--259},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {zyq49wV5zA},
	abstract = {Contagion effect refers to the causal effect of peer behavior on the outcome of an individual in social networks. Contagion can be hard to estimate when it is confounded by latent homophily because nodes in a homophilic network tend to have ties to peers with similar attributes and can behave similarly without influencing one another. One way to account for latent homophily is by considering proxies for the unobserved confounders. However, as we demonstrate in this paper, existing proxy-based methods for contagion effect estimation have a very high variance when the proxies are high-dimensional. To address this issue, we introduce a novel framework, Proximal Embeddings (ProEmb), that integrates variational autoencoders with adversarial networks to create low-dimensional representations of high-dimensional proxies and help with estimating contagion effects. While VAEs have been used previously for representation learning in causal inference, a novel aspect of our approach is the additional component of adversarial networks to balance the representations of different treatment groups, which is essential in causal inference from observational data where these groups typically come from different distributions. We empirically show that our method significantly increases the accuracy and reduces the variance of contagion effect estimation in observational network data compared to state-of-the-art methods. We also demonstrate its applicability to two real-world scenarios, estimating contagion on social media and in adolescent smoking behavior.}
}

@InProceedings{pmlr-v275-lee25a,
	title = {Matchings, Predictions and Counterfactual Harm in Refugee Resettlement Processes},
	author = {Lee, Seungeon and Corvelo Benz, Nina L. and Thejaswi, Suhas and Gomez-Rodriguez, Manuel},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {260--291},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {FBN6AWBSgF},
	abstract = {Resettlement agencies have started to adopt data-driven algorithmic matching to match refugees to locations using employment rate as a measure of utility. Given a pool of refugees, data-driven algorithmic matching utilizes a classifier to predict the probability that each refugee would find employment at any given location. Then, it uses the predicted probabilities to estimate the expected utility of all possible placement decisions. Finally, it finds the placement decisions that maximize the predicted utility by solving a maximum weight bipartite matching problem. In this work, we argue that, using existing solutions, there may be pools of refugees for which data-driven algorithmic matching is (counterfactually) harmful\textemdash it would have achieved lower utility than a given default policy used in the past, had it been used. Then, we develop a post-processing algorithm that, given placement decisions made by a default policy on a pool of refugees and the employment outcomes of the refugees in the pool, solves an inverse matching problem to minimally modify the predictions made by a given classifier. Under these modified predictions, the optimal matching policy that maximizes predicted utility on the pool is guaranteed to be not harmful. Further, we introduce a Transformer model that, given placement decisions made by a default policy on multiple pools of refugees and the employment outcomes of the refugees in these pools, learns to modify the predictions made by a given classifier so that the optimal matching policy that maximizes predicted utility under the modified predictions on an unseen pool of refugees is less likely to be harmful than under the original predictions. Experiments on simulated resettlement processes using synthetic refugee data created from a variety of publicly available data from international organizations, including the United Nations Refugee Agency (UNHCR), suggest that our methodology may be effective in making algorithmic placement decisions that are less likely to be harmful than existing solutions.}
}

@InProceedings{pmlr-v275-russo25a,
	title = {Shapley-PC: Constraint-based Causal Structure Learning with a Shapley Inspired Framework},
	author = {Russo, Fabrizio and Toni, Francesca},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {292--339},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {DkQkyuvPhx},
	abstract = {Causal Structure Learning (CSL), also referred to as causal discovery, amounts to extracting causal relations among variables in data. CSL enables the estimation of causal effects from observational data alone, avoiding the need to perform real life experiments. Constraint-based CSL leverages conditional independence tests to perform causal discovery. We propose Shapley-PC, a novel method to improve constraint-based CSL algorithms by using Shapley values over the possible conditioning sets, to decide which variables are responsible for the observed conditional (in)dependences. We prove soundness, completeness and asymptotic consistency of Shapley-PC and run a simulation study showing that our proposed algorithm is superior to existing versions of PC.}
}

@InProceedings{pmlr-v275-bayer25a,
	title = {Fair Clustering: A Causal Perspective},
	author = {Bayer, Fritz and Ple\v{c}ko, Drago and Beerenwinkel, Niko and Kuipers, Jack},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {340--358},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {Ek1U1G5dW5},
	abstract = {Clustering algorithms may unintentionally propagate or intensify existing disparities, leading to unfair representations or biased decision-making. Current fair clustering methods rely on notions of fairness that do not capture any information on the underlying causal mechanisms. We show that optimising for non-causal fairness notions can paradoxically induce direct discriminatory effects from a causal standpoint. We present a clustering approach that incorporates causal fairness metrics to provide a more nuanced approach to fairness in unsupervised learning. Our approach enables the specification of the causal fairness metrics that should be minimised. We demonstrate the efficacy of our methodology using datasets known to harbour unfair biases.}
}

@InProceedings{pmlr-v275-oesterle25a,
	title = {Beyond Single-Feature Importance with ICECREAM},
	author = {Oesterle, Michael and Bl\"{o}baum, Patrick and Mastakouri, Atalanti A. and Kirschbaum, Elke},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {359--389},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {GCABMVe1E4},
	abstract = {Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods that rank individual factors. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.}
}

@InProceedings{pmlr-v275-hines25a,
	title = {Automatic debiasing of neural networks via moment-constrained learning},
	author = {Hines, Christian L. and Hines, Oliver J.},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {390--405},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {GbCz7FMXoF},
	abstract = {Causal and nonparametric estimands in economics and biostatistics can often be viewed as the mean of a linear functional applied to an unknown outcome regression function.  Naively learning the regression function and taking a sample mean of the target functional results in biased estimators, and a rich debiasing literature has developed where one additionally learns the so-called Riesz representer (RR) of the target estimand (targeted learning, double ML, automatic debiasing etc.). Learning the RR via its derived functional form can be challenging, e.g. due to extreme inverse probability weights or the need to learn conditional density functions. Such challenges have motivated recent advances in automatic debiasing (AD), where the RR is learned directly via minimization of a bespoke loss. We propose moment-constrained learning as a new RR learning approach that addresses some shortcomings in AD, constraining the predicted moments and improving the robustness of RR estimates to optimization hyperparamters. Though our approach is not tied to a particular class of learner, we illustrate it using neural networks, and evaluate on the problems of average treatment/derivative effect estimation using semi-synthetic data. Our numerical experiments show improved performance versus state of the art benchmarks.}
}

@InProceedings{pmlr-v275-popescu25a,
	title = {Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation},
	author = {Popescu, Oana-Iuliana and Gerhardus, Andreas and Rabel, Martin and Runge, Jakob},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {406--450},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {4shh1euUgx},
	abstract = {Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables in a dataset are of the same type, either numerical or categorical, many real-world applications involve mixed-type datasets that include both numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these estimators and propose a variation of the former approach that does not treat categorical variables as numeric. Our extensive numerical experiments show that our variant detects dependencies more robustly across different data distributions and preprocessing types.}
}

@InProceedings{pmlr-v275-norimatsu25a,
	title = {Encode-Decoder-based GAN for Estimating Counterfactual Outcomes under Sequential Selection Bias and Combinatorial Explosion},
	author = {Norimatsu, Yoshiyuki and Imaizumi, Masaaki},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {451--489},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {4RpF1eosDS},
	abstract = {Estimating counterfactual outcomes of time-varying treatment types and associated dosages is important for addressing medical problems.
    This task becomes more challenging when both the treatment type and dosage assignment are biased due to the presence of time-varying confounders, 
    as compared to estimating outcomes for treatment types alone.
    Specifically, the setup yields the following two obstacles: 
    first, treatment types and dosages are selected sequentially, causing observed outcomes to be biased at each time step, 
    leading to $2 \times \tau$ biases for a $\tau$-step-ahead prediction (sequential selection bias);  
    second, the number of treatment trajectories increases exponentially with $\tau$ (combinatorial explosion). 
    In this paper, we introduce Encoder-Decoder Time-SCIGAN (EDTS), 
    which combines a longitudinal encoder-decoder transformer with a Generative Adversarial Network (GAN) for estimating counterfactuals.
    The encoder-decoder architecture predicts outcomes for one-step- and multi-step-ahead predictions separately, 
    while the GAN generates counterfactual outcomes that cannot be distinguished from observed outcomes by the discriminators to handle sequential selection bias.
    To address combinatorial explosion, we propose a novel discrimination method, 
    Sequential Counterfactual Discrimination (SCD) for EDTS discriminators.
    Our evaluation of synthetic and semi-synthetic datasets demonstrate that EDTS outperforms the current baselines. 
    To the best of our knowledge, this is the first study to propose an architecture 
    for estimating counterfactual outcomes of both time-varying treatment types and dosages.
    Implementation is available at \url{https://github.com/ynorimat/EDTS}.}
}

@InProceedings{pmlr-v275-pandeva25a,
	title = {Robust Multi-view Co-expression Network Inference},
	author = {Pandeva, Teodora and Jonker, Martijs Johannes and Hamoen, Leendert and Mooij, Joris and Forr\'{e}, Patrick},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {490--513},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {rlygvcqSzX},
	abstract = {Unraveling the co-expression of genes across studies enhances the understanding of cellular processes. Inferring gene co-expression networks from transcriptome data presents many challenges, including the high-dimensionality of the data relative to the number of samples, sample correlations, and batch effects. To address these complexities, we introduce a robust method for high-dimensional graph inference from multiple independent studies.  We base our approach on the premise that each dataset is essentially a noisy linear mixture of gene loadings that follow a multivariate $t$-distribution with a sparse precision matrix, which is shared across studies. This allows us to show that  we can identify the co-expression matrix up to a scaling factor among other model parameters. Our method employs an Expectation-Maximization procedure for parameter estimation. Empirical evaluation on synthetic and gene expression data demonstrates our method's improved ability to learn the underlying graph structure compared to baseline methods.}
}

@InProceedings{pmlr-v275-beckers25a,
	title = {Actual Causation and Nondeterministic Causal Models},
	author = {Beckers, Sander},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {514--532},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {rfcXzNeRqH},
	abstract = {In (Beckers, 2025) I introduced nondeterministic causal models as a generalization of Pearl's standard deterministic causal models. I here take advantage of the increased expressivity offered by these models to offer a novel definition of actual causation (that also applies to deterministic models). Instead of motivating the definition by way of (often subjective) intuitions about examples, I proceed by developing it based entirely on the unique function that it can fulfil in communicating and learning a causal model. First I generalize the more basic notion of counterfactual dependence, second I show how this notion has a vital role to play in the logic of causal discovery, third I introduce the notion of a structural simplification of a causal model, and lastly I bring both notions together in my definition of actual causation. Although novel, the resulting definition arrives at verdicts that are almost identical to those of my previous definition (Beckers, 2021, 2022).}
}

@InProceedings{pmlr-v275-chevalley25a,
	title = {The CausalBench challenge: A machine learning contest for gene network inference from single-cell perturbation data},
	author = {Chevalley, Mathieu and Sackett-Sanders, Jacob and Roohani, Yusuf H and Notin, Pascal and Bakulin, Artemy and Brzezinski, Dariusz and Deng, Kaiwen and Guan, Yuanfang and Hong, Justin and Ibrahim, Michael and Kotlowski, Wojciech and Kowiel, Marcin and Misiakos, Panagiotis and Nazaret, Achille and P\"{u}schel, Markus and Wendler, Chris and Mehrjou, Arash and Schwab, Patrick},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {533--551},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {3HXpGDZgii},
	abstract = {In drug discovery, mapping interactions between genes within cellular systems is a crucial early step. Such maps are not only foundational for understanding the molecular mechanisms underlying disease biology but also pivotal for formulating hypotheses about potential targets for new medicines. Recognizing the need to elevate the construction of these gene-gene interaction networks, especially from large-scale, real-world datasets of perturbed single cells, the CausalBench Challenge was initiated. This challenge aimed to inspire the machine learning community to enhance state-of-the-art methods, emphasizing better utilization of expansive genetic perturbation data. Using the framework provided by the CausalBench benchmark, participants were tasked with refining the current methodologies or proposing new ones. This report provides an analysis and summary of the methods submitted during the challenge to give a partial image of the state of the art at the time of the challenge. Notably, the winning solutions significantly improved performance compared to previous baselines, establishing a new state of the art for this critical task in biology and medicine.}
}

@InProceedings{pmlr-v275-montagna25a,
	title = {Score matching through the roof: linear, nonlinear, and latent variables causal discovery},
	author = {Montagna, Francesco and Faller, Philipp Michael and Bl\"{o}baum, Patrick and Kirschbaum, Elke and Locatello, Francesco},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {552--605},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {HNMuBz1JXO},
	abstract = {Causal discovery from observational data holds great promise, but existing methods rely on strong assumptions about the underlying causal structure, often requiring full observability of all relevant variables. We tackle these challenges by leveraging the score function $\nabla \log p(X)$ of observed variables for causal discovery and propose the following contributions. First, we generalize the existing results of identifiability with the score to additive noise models with minimal requirements on the causal mechanisms. Second, we establish conditions for inferring causal relations from the score even in the presence of hidden variables; this result is two-faced: we demonstrate the score's potential as an alternative to conditional independence tests to infer the equivalence class of causal graphs with hidden variables, and we provide the necessary conditions for identifying direct causes in latent variable models. Building on these insights, we propose a flexible algorithm for causal discovery across linear, nonlinear, and latent variable models, which we empirically validate.}
}

@InProceedings{pmlr-v275-sick25a,
	title = {Interpretable Neural Causal Models with TRAM-DAGs},
	author = {Sick, Beate and D\"{u}rr, Oliver},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {606--630},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {h3DDKFCOTq},
	abstract = {The ultimate goal of most scientific studies is to understand the underlying causal mechanism between the involved variables. Structural causal models (SCMs) are widely used to represent such causal mechanisms. Given an SCM, causal queries on all three levels of Pearl's causal hierarchy can be answered: $\mathcal{L}_1$ observational, $\mathcal{L}_2$ interventional, and $\mathcal{L}_3$ counterfactual.
An essential aspect of modeling the SCM is to model the dependency of each variable on its causal parents. Traditionally this is done by parametric statistical models, such as linear or logistic regression models. This allows to handle all kinds of data types and fit interpretable models but bears the risk of introducing a bias due to the assumed rigid functional form. More recently neural causal models came up using neural networks (NNs) to model the causal relationships, allowing the estimation of nearly any underlying functional form without bias. However, current neural causal models are generally restricted to continuous variables and do not yield an interpretable form of the causal relationships. Transformation models range from simple statistical regressions to complex networks and can handle continuous, ordinal, and binary data. Here, we propose to use TRAMs to model the functional relationships in SCMs allowing us to
bridge the gap between interpretability and flexibility in causal modeling. We call this method TRAM-DAG and assume currently that the underlying directed acyclic graph (DAG) is known. For the fully observed case, we benchmark TRAM-DAGs against state-of-the-art statistical and NN-based causal models. We show that TRAM-DAGs are interpretable but also achieve equal or superior performance in queries ranging from $\mathcal{L}_1$ to $\mathcal{L}_3$ in the causal hierarchy.  For the continuous case, TRAM-DAGs allow for counterfactual queries for three common causal structures, including unobserved confounding.}
}

@InProceedings{pmlr-v275-rios25a,
	title = {Exact discovery is polynomial for certain sparse causal Bayesian networks},
	author = {Rios, Felix Leopoldo and Moffa, Giusi and Kuipers, Jack},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {631--658},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {I1CTszODef},
	abstract = {Causal Bayesian networks are widely used tools for summarising the dependencies between variables and elucidating their putative causal relationships. By restricting the search to trees, for example, learning the optimum from data is polynomial, but this does not guarantee finding the optimal network overall. Without similar restrictions, exact discovery of the optimum is computationally hard in general and no polynomial results are known. The current state-of-the-art approaches are integer linear programming over the underlying space of directed acyclic graphs, dynamic programming and shortest-path searches over the space of topological orders, and constraint programming combining both. For dynamic programming over orders, the computational complexity is known to be exponential base 2 in the number of variables in the network. We demonstrate how to use properties of Bayesian networks to prune the search space and lower the computational cost, while still guaranteeing exact discovery of the provably optimal network. We also include new path-search and divide-and-conquer criteria.  Without a priori constraining the search to certain types of networks, the algorithm completes in quadratic time when the optimum is a matching, and in polynomial time when the optimum belongs to any network class with logarithmically-bound largest connected components. In simulation studies we observe the polynomial dependence for sparse networks and that, beyond some critical value, the logarithm of the base grows with the network density. Our approach then out-competes the state-of-the-art at lower densities. These results therefore pave the way for faster exact causal discovery in larger and sparser networks.}
}

@InProceedings{pmlr-v275-schkoda25a,
	title = {Cross-validating causal discovery via Leave-One-Variable-Out},
	author = {Schkoda, Daniela and Faller, Philipp Michael and Janzing, Dominik and Bl\"{o}baum, Patrick},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {659--692},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {VO4UpZHMpf},
	abstract = {We propose a new approach to falsify causal discovery algorithms without ground truth, which is based on testing the causal model on a variable pair excluded during learning the causal model. Specifically, given data on $X, Y, \boldsymbol{Z}=X, Y, Z_1,\dots,Z_k$, we apply the causal discovery algorithm separately to the 'leave-one-out' data sets $X, \boldsymbol{Z}$ and $Y, \boldsymbol{Z}$. We demonstrate that the two resulting causal models, in the form DAGs, ADMGs, CPDAGs or PAGs, often entail conclusions on the dependencies between $X$ and $Y$ and allow to estimate $\mathbb{E}(Y\mid X=x)$ without any joint observations of $X$ and $Y$, given only the leave-one-out datasets. This estimation is called 
 "Leave-One-Variable-Out (LOVO)" prediction. Its error can be  estimated since the joint distribution $P(X, Y)$ is available, and $X$ and $Y$ have only been omitted for the purpose of falsification.
 
We present two variants of LOVO prediction: One graphical method, which is applicable to general causal discovery algorithms, and one version tailored towards algorithms relying on specific a priori assumptions, such as linear additive noise models. 
Simulations indicate that the LOVO prediction error is indeed correlated with the accuracy of the causal outputs, affirming the method's effectiveness.}
}

@InProceedings{pmlr-v275-pena25a,
	title = {Bounds and Sensitivity Analysis of the Causal Effect Under Outcome-Independent MNAR Confounding},
	author = {Pe\~{n}a, Jose},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {693--703},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {qDmC2Et39f},
	abstract = {We report distribution-free bounds for any contrast between the probabilities of the potential outcome under exposure and non-exposure when the confounders are missing not at random. We assume that the missingness mechanism is outcome-independent. We also report a sensitivity analysis method to complement our bounds.}
}

@InProceedings{pmlr-v275-schooltink25a,
	title = {Aligning Graphical and Functional Causal Abstractions},
	author = {Schooltink, Willem and Zennaro, Fabio Massimo},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {704--730},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {itESdHtCyO},
	abstract = {Causal abstractions allow us to relate causal models on different levels of granularity. To ensure that the models agree on cause and effect, frameworks for causal abstractions define notions of consistency. Two distinct methods for causal abstraction are common in the literature: (i) graphical abstractions, such as Cluster DAGs, which relate models on a structural level, and (ii) functional abstractions, like $\alpha$-abstractions, which relate models by maps between variables and their ranges. In this paper we will align the notions of graphical and functional consistency and show an equivalence between the class of Cluster DAGs, consistent $\alpha$-abstractions, and constructive $\tau$-abstractions. Furthermore, we extend this alignment by introducing more expressive Partial Cluster DAGs. Our results provide a rigorous bridge between the functional and graphical frameworks and allow for adoption and transfer of results between them.}
}

@InProceedings{pmlr-v275-deng25a,
	title = {Transfer learning in latent contextual bandits with covariate shift through causal transportability},
	author = {Deng, Mingwei and Kyrki, Ville and Baumann, Dominik},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {731--756},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {7HFjvcdNOk},
	abstract = {Transferring knowledge from one environment to another is an essential ability of intelligent systems. Nevertheless, when two environments are different, naively transferring all knowledge may deteriorate the performance, a phenomenon known as negative transfer. In this paper, we address this issue within the framework of multi-armed bandits from the perspective of causal inference. Specifically, we consider transfer learning in latent contextual bandits, where the actual context is hidden, but a potentially high-dimensional proxy is observable. We further consider a covariate shift in the context across environments. We show that naively transferring all knowledge for classical bandit algorithms in this setting led to negative transfer. We then leverage transportability theory from causal inference to develop algorithms that explicitly transfer effective knowledge for estimating the causal effects of interest in the target environment. Besides, we utilize variational autoencoders to approximate causal effects under the presence of a high-dimensional proxy. We test our algorithms on synthetic and semi-synthetic datasets, empirically demonstrating consistently improved learning efficiency across different proxies compared to baseline algorithms, showing the effectiveness of our causal framework in transferring knowledge.}
}

@InProceedings{pmlr-v275-mhasawade25a,
	title = {Disparate Effect Of Missing Mediators On Transportability of Causal Effects},
	author = {Mhasawade, Vishwali and Chunara, Rumi},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {757--771},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {rr0FDBq5XR},
	abstract = {The transport of mediation effects is an important question for upstream interventions, such
as targeted development of parks to improve population health, as their effect on populations
is mediated by factors like physical activity, which can vary from place to place. However,
upstream treatment effect estimates could be biased when mediator variables are missing for the
population where the effect is to be transported. We study this issue of the impact of missing
mediators on transported effects, motivated by challenges in public health, wherein mediators are
commonly missing but not at random. We propose a sensitivity analysis framework to quantify
the impact of missing mediator data on transported mediation effects, identifying when the
conditional transported mediation effect becomes insignificant for subgroups with missing data.
Applied to longitudinal data from the Moving to Opportunity Study, a large-scale housing voucher
experiment, this framework demonstrates the sensitivity of transported mediation effects to data
missingness. In particular, we quantify the effect of missing mediators on transport effect estimates
of voucher receipt in childhood, an upstream intervention on living location. We then assess the
subsequent impact on the risk of mental health or substance use disorder mediated through parental
health across sites. Our findings highlight that missing mediators can disparately impact effect
estimates across population subgroups and provide a tangible understanding of how much missing
data can be withstood for unbiased effect estimates in such mediated settings.}
}

@InProceedings{pmlr-v275-kuipers25a,
	title = {The interventional Bayesian Gaussian equivalent score for Bayesian causal inference with unknown soft interventions},
	author = {Kuipers, Jack and Moffa, Giusi},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {772--791},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {T4y0vAnTwO},
	abstract = {Describing the causal relations governing a system is a fundamental task in many scientific fields, ideally addressed by experimental studies. However, obtaining data under intervention scenarios may not always be feasible, while discovering causal relations from purely observational data is notoriously challenging. In certain settings, such as genomics, we may have data from heterogeneous study conditions, with soft (partial) interventions only pertaining to a subset of the study variables, whose effects and targets are possibly unknown. Combining data from experimental and observational studies offers the opportunity to leverage both domains and improve the identifiability of causal structures. To this end, we define the interventional BGe score for a mixture of observational and interventional data for linear-Gaussian models, where the targets and effects of intervention may be unknown. Prerogative of our method is that it takes a Bayesian perspective leading to a full characterisation of the posterior distribution of the DAG structures. Given a sample of DAGs, one can also automatically derive full posterior distributions of the intervention effects. Consequently, the method effectively captures the uncertainty both in the structure and the parameter estimates. We additionally demonstrate the performance of the approach both in simulations and data analysis applications.  Codes to reproduce the simulations and analyses are publicly available at https://github.com/jackkuipers/iBGe.}
}

@InProceedings{pmlr-v275-kazemi25a,
	title = {Counterfactual Influence in Markov Decision Processes},
	author = {Kazemi, Milad and Lally, Jessica and Tishchenko, Ekaterina and Chockler, Hana and Paoletti, Nicola},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {792--817},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {KdDfPic5QR},
	abstract = {Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs). Given an MDP path $\tau$, counterfactual inference allows us to derive counterfactual paths $\tau'$ describing _what-if_ versions of $\tau$ obtained under different action sequences than those observed in $\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, _the observation $\tau$ may no longer influence the counterfactual world_, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones. This issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, and yet, it has remained overlooked until now. In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions. We devise an algorithm to construct counterfactual models that automatically satisfy influence constraints. Leveraging such models, we derive counterfactual policies that are not just optimal for a given reward structure but also remain tailored to the observed path. Even though there is an unavoidable trade-off between policy optimality and strength of influence constraints, our experiments demonstrate that it is possible to derive (near-)optimal policies while remaining under the influence of the observation.}
}

@InProceedings{pmlr-v275-mazaheri25a,
	title = {Omitted Labels Induce Nontransitive Paradoxes in Causality},
	author = {Mazaheri, Bijan and Jain, Siddharth and Cook, Matthew and Bruck, Jehoshua},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {818--833},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {rUTEulcfe5},
	abstract = {We explore "omitted label contexts," in which training data is limited to a subset of the possible labels. This setting is standard among specialized human experts or specific focused studies. By studying Simpson's paradox, we observe that "correct" adjustments sometimes require non-exchangeable treatment and control groups. A generalization of Simpson's paradox leads us to study networks of conclusions drawn from different contexts, within which a paradox of nontransitivity arises. We prove that the space of possible nontransitive structures in these networks exactly corresponds to structures that form from aggregating ranked-choice votes.}
}

@InProceedings{pmlr-v275-brouillard25a,
	title = {The Landscape of Causal Discovery Data: Grounding Causal Discovery in Real-World Applications},
	author = {Brouillard, Philippe and Squires, Chandler and Wahl, Jonas and K"{o}rding, Konrad and Sachs, Karen and Drouin, Alexandre and Sridhar, Dhanya},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {834--873},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {Gl5I7k84zC},
	abstract = {Causal discovery aims to automatically uncover causal relationships from data, a capability with significant potential across many scientific disciplines. However, its real-world applications remain limited. Current methods often rely on unrealistic assumptions and are evaluated only on simple synthetic toy datasets, often with inadequate evaluation metrics. In this paper, we substantiate these claims by performing a systematic review of the recent causal discovery literature. We present applications in biology, neuroscience, and Earth sciences\textemdash fields where causal discovery holds promise for addressing key challenges. We highlight available simulated and real-world datasets from these domains and discuss common assumption violations that have spurred the development of new methods. Our goal is to encourage the community to adopt better evaluation practices by utilizing realistic datasets and more adequate metrics.}
}

@InProceedings{pmlr-v275-azua25a,
	title = {The Causal-Effect Score in Data Management},
	author = {Az\'{u}a, Felipe and Bertossi, Leopoldo},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {874--893},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {szTYDG3Omr},
	abstract = {The Causal Effect (CE) is a numerical measure of causal influence of variables on observed results.
Despite being widely used in many areas, only preliminary attempts have been made to use CE as an
attribution score in data management, to measure the causal strength of tuples for query answering
in databases. In this work, we introduce, generalize and investigate the so-called Causal-Effect
Score in the context of classical and probabilistic databases.}
}

@InProceedings{pmlr-v275-zhu25a,
	title = {Optimizing Multi-Scale Representations to Detect Effect Heterogeneity Using Earth Observation and Computer Vision: Applications to Two Anti-Poverty RCTs},
	author = {Zhu, Fucheng Warren and Jerzak, Connor Thomas and Daoud, Adel},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {894--919},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {C6pMjWF343},
	abstract = {Earth Observation (EO) data are increasingly used in policy analysis by enabling granular estimation of conditional average treatment effects (CATE). However, a challenge in EO-based causal inference is determining the scale of the input satellite imagery---balancing the trade-off between capturing fine-grained individual heterogeneity in smaller images and broader contextual information in larger ones. This paper introduces Multi-Scale Representation Concatenation, a set of composable procedures that transform arbitrary single-scale EO-based CATE estimation algorithms into multi-scale ones. We benchmark the performance of Multi-Scale Representation Concatenation on a CATE estimation pipeline that combines Vision Transformer (ViT) models (which encode images) with Causal Forests (CFs) to obtain CATE estimates from those encodings. We first perform simulation studies where the causal mechanism is known, showing that our multi-scale approach captures information relevant to effect heterogeneity that single-scale ViT models fail to capture as measured by R-squared. We then apply the multi-scale method to two randomized controlled trials (RCTs) conducted in Peru and Uganda using Landsat satellite imagery. As we do not have access to ground truth CATEs in the RCT analysis, the Rank Average Treatment Effect Ratio (RATE Ratio) measure is employed to assess performance. Results indicate that Multi-Scale Representation Concatenation improves the performance of deep learning models in EO-based CATE estimation without the complexity of designing new multi-scale architectures for a specific use case. The application of Multi-Scale Representation Concatenation could have meaningful policy benefits---e.g., potentially increasing the impact of poverty alleviation programs without additional resource expenditure.}
}

@InProceedings{pmlr-v275-esponera25a,
	title = {Inducing Causal Structure Applied to Glucose Prediction for T1DM Patients},
	author = {Esponera, Ana and Cin\`{a}, Giovanni},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {920--946},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {mlxtcyrgWc},
	abstract = {Causal abstraction techniques such as Interchange Intervention Training (IIT) have been proposed to infuse neural network with expert knowledge encoded in causal models, but their application to real-world problems remains limited. This article explores the application of IIT in predicting blood glucose levels in Type 1 Diabetes Mellitus (T1DM) patients. The study utilizes an acyclic version of the simglucose simulator approved by the FDA to train a Multi-Layer Perceptron (MLP) model, employing IIT to impose causal relationships. Results show that the model trained with IIT effectively abstracted the causal structure and outperformed the standard one in terms of predictive performance across different prediction horizons (PHs) post-meal. Furthermore, the breakdown of the counterfactual loss can be leveraged to explain which part of the causal mechanism are more or less effectively captured by the model. These preliminary results suggest the potential of IIT in enhancing predictive models in healthcare by effectively complying with expert knowledge.}
}

@InProceedings{pmlr-v275-kornai25a,
	title = {AGM-TE: Approximate Generative Model Estimator of Transfer Entropy for Causal Discovery},
	author = {Kornai, Daniel and Silva, Ricardo and Nikolaou, Nikolaos},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {947--990},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {w2vMTPxI6y},
	abstract = {The discovery of causal interactions from time series data is an increasingly common approach in science and engineering. Many of the approaches for solving it rely on an information-theoretic measure called transfer entropy [TE] to infer directed causal interactions. However, TE is difficult to estimate from empirical data, as non-parametric methods are hindered by the curse of dimensionality, while existing ML methods suffer from slow convergence or overfitting. 
  
  In this work, we introduce AGM-TE, a novel ML method that estimates TE using the difference in the predictive capabilities of two alternative probabilistic forecasting models. In a comprehensive suite of TE estimation benchmarks [with 100+ tasks], AGM-TE achieves SoTA results in terms of accuracy and data efficiency when compared to existing non-parametric and ML estimators. AGM-TE further differentiates itself with the ability to estimate conditional transfer entropy, which helps mitigate the effect of confounding variables in systems with many interacting components. We demonstrate the strengths of our approach empirically by recovering patterns of brain connectivity from 250+ dimensional spike data that are consistent with known neuroanatomical results. 
  
  Overall, we believe AGM-TE represents a significant step forward in the application of transfer entropy to problems of causal discovery from observational time series data.}
}

@InProceedings{pmlr-v275-burauel25a,
	title = {Controlling for discrete unmeasured confounding in nonlinear causal models},
	author = {Burauel, Patrick and Eberhardt, Frederick and Besserve, Michel},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {991--1015},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {kAkniomGMF},
	abstract = {Unmeasured confounding is a major challenge for identifying causal relationships from non-experimental data. Here, we propose a method that can accommodate unmeasured discrete confounding. Extending recent identifiability results in deep latent variable models, we show theoretically that confounding can be detected and corrected under the assumption that the observed data is a piecewise affine transformation of a latent Gaussian mixture model and that the identity of the mixture components is confounded. We provide a flow-based algorithm to estimate this model and perform deconfounding. Experimental results on synthetic and real-world data provide support for the effectiveness of our approach.}
}

@InProceedings{pmlr-v275-oriordan25a,
	title = {Local Interference: Removing Interference Bias in Semi-Parametric Causal Models},
	author = {O'Riordan, Michael and Gilligan-Lee, Ciar\'{a}n Mark},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1016--1031},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {0TDacAmjGK},
	abstract = {Interference bias is a major impediment to identifying causal effects in real-world settings. For example, vaccination reduces the transmission of a virus in a population such that everyone benefits\textemdash even those who are not treated. This is a source of bias that must be accounted for if one wants to learn the true effect of a vaccine on an individual's immune system. Previous approaches addressing interference bias require strong domain knowledge in the form of a graphical interaction network fully describing interference between units. Moreover, they place additional constraints on the form the interference can take, such as restricting to linear outcome models, and assuming that interference experienced by a unit does not depend on the unit's covariates. Our work addresses these shortcomings. We first provide and justify a novel definition of causal models with local interference. We prove that the True Average Causal Effect, a measure of causality where interference has been removed, can be identified in certain semi-parametric models satisfying this definition. These models allow for non-linearity, and also for interference to depend on a unit's covariates. An analytic estimand for the True Average Causal Effect is given in such settings. We further prove that the True Average Causal Effect cannot be identified in arbitrary models with local interference, showing that identification requires semi-parametric assumptions. Finally, we provide an empirical validation of our method on both simulated and real-world datasets.}
}

@InProceedings{pmlr-v275-choo25a,
	title = {Probably approximately correct high-dimensional causal effect estimation given a valid adjustment set},
	author = {Choo, Davin and Squires, Chandler and Bhattacharyya, Arnab and Sontag, David},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1032--1085},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {nRDkJOczJS},
	abstract = {Accurate estimates of causal effects play a key role in decision-making across applications such as healthcare, economics, and operations. In the absence of randomized experiments, a common approach to estimating causal effects uses covariate adjustment. In this paper, we study covariate adjustment for discrete distributions from the PAC learning perspective, assuming knowledge of a valid adjustment set $\mathbf{Z}$, which might be high-dimensional. Our first main result PAC-bounds the estimation error of covariate adjustment by a term that is exponential in the size of the adjustment set; it is known that such a dependency is unavoidable even if one only aims to minimize the mean squared error. Motivated by this result, we introduce the notion of an $\varepsilon$-Markov blanket, give bounds on the misspecification error of using such a set for covariate adjustment, and provide an algorithm for $\varepsilon$-Markov blanket discovery; our second main result upper bounds the sample complexity of this algorithm. Furthermore, we provide a misspecification error bound and a constraint-based algorithm that allow us to go beyond $\varepsilon$-Markov blankets to even smaller adjustment sets. Our third main result upper bounds the sample complexity of this algorithm, and our final result combines the first three into an overall PAC bound. Altogether, our results highlight that one does not need to perfectly recover causal structure in order to ensure accurate estimates of causal effects.}
}

@InProceedings{pmlr-v275-pradier25a,
	title = {Beyond Flatland: A Geometric Take on Matching Methods for Treatment Effect Estimation},
	author = {Pradier, Melanie F. and Gonz\'{a}lez, Javier},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1086--1115},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {n3kAuIGkbN},
	abstract = {Matching is a popular approach in causal inference to estimate treatment effects by pairing treated and control units that are most similar in terms of their covariate information. However, classic matching methods completely ignore the geometry of the data manifold, which is crucial to define a meaningful distance for matching, and struggle when covariates are noisy and high-dimensional. In this work, we propose GeoMatching, a matching method to estimate treatment effects that takes into account the intrinsic data geometry induced by existing causal mechanisms among the confounding variables. First, we learn a low-dimensional, latent Riemannian manifold that accounts for uncertainty and geometry of the original input data. Second, we estimate treatment effects via matching in the latent space based on the learned latent Riemannian metric. We provide theoretical insights and empirical results in synthetic and real-world scenarios, demonstrating that GeoMatching yields more effective treatment effect estimators, even as we increase input dimensionality, in the presence of outliers, or in semi-supervised scenarios.}
}

@InProceedings{pmlr-v275-bang25a,
	title = {Constraint-based causal discovery with tiered background knowledge and latent variables in single or overlapping datasets},
	author = {Bang, Christine W. and Didelez, Vanessa},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1116--1146},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {8obpn6giCd},
	abstract = {In this paper we consider the use of tiered background knowledge within constraint based causal discovery. Our focus is on settings relaxing causal sufficiency, i.e. allowing for latent variables which may arise because relevant information could not be measured at all, or not jointly, as in the case of multiple overlapping datasets. We first present novel insights into the properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce a new extension of the IOD (integrating overlapping datasets) algorithm incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm. We show that under full usage of the tiered background knowledge tFCI and tIOD are sound, while simple versions of the tIOD and tFCI are sound and complete. We further show that the tIOD algorithm can often be expected to be considerably more efficient and informative than the IOD algorithm even beyond the obvious restriction of the Markov equivalence classes. We provide a formal result on the conditions for this gain in efficiency and informativeness. Our results are accompanied by a series of examples illustrating the exact role and usefulness of tiered background knowledge.}
}

@InProceedings{pmlr-v275-wu25a,
	title = {Network Causal Effect Estimation In Graphical Models Of Contagion And Latent Confounding},
	author = {Wu, Yufeng and Bhattacharya, Rohit},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1147--1173},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {7NdZ4CV6zA},
	abstract = {A key question in many network studies is whether the observed correlations between units are primarily due to contagion  or latent confounding. Here, we study this question using a segregated graph (Shpitser, 2015) representation of these mechanisms, and examine how uncertainty about the true underlying mechanism impacts downstream computation of network causal effects, particularly under full interference---settings where we only have a single realization of a network and each unit may depend on any other unit in the network. Under certain assumptions about asymptotic growth of the network,  we derive likelihood ratio tests that can be used to identify whether different sets of variables---confounders, treatments, and outcomes---across units exhibit dependence due to contagion or latent confounding. We then propose network causal effect estimation strategies  that provide unbiased and consistent estimates if the dependence mechanisms are either known or correctly inferred using our proposed tests. Together, the proposed methods allow network effect estimation in a wider range of full interference scenarios that have not been considered in prior work. We evaluate the effectiveness of our methods with synthetic data and the validity of our assumptions using real-world networks.}
}

@InProceedings{pmlr-v275-gao25a,
	title = {Counterfactual explanability of black-box prediction models},
	author = {Gao, Zijun and Zhao, Qingyuan},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1174--1174},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {ikDm0W4mzt},
	abstract = {It is crucial to be able to explain black-box prediction models to use them effectively and safely in practice. Most existing tools for model explanations are associational rather than causal, and we use two paradoxical examples to show that such explanations are generally inadequate. Motivated by the concept of genetic heritability in twin studies, we propose a new notion called counterfactual explainability for black-box prediction models. Counterfactual explainability has three key advantages: (1) it leverages counterfactual outcomes and extends methods for global sensitivity analysis (such as functional analysis of variance and Sobol's indices) to a causal setting; (2) it is defined not only for the totality of a set of input factors but also for their interactions (indeed, it is a probability measure on a whole "explanation algebra'"); (3) it also applies to dependent input factors whose causal relationship can be modeled by a directed acyclic graph, thus incorporating causal mechanisms into the explanation.}
}

@InProceedings{pmlr-v275-zhang25a,
	title = {MXMap: A Multivariate Cross Mapping Framework for Causal Discovery in Dynamical Systems},
	author = {Zhang, Elise and Mirall\`{e}s, Fran\c{c}ois and Rousseau-Rizzi, Rapha\"{e}l and Zinflou, Arnaud and Wu, Di and Boulet, Benoit},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1175--1216},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {vaT6eAyKi3},
	abstract = {Convergent Cross Mapping (CCM) is a powerful method for detecting causality in coupled nonlinear dynamical systems, providing a model-free approach to capture dynamic causal interactions. Partial Cross Mapping (PCM) was introduced as an extension of CCM to address indirect causality in three-variable systems by comparing cross-mapping quality between direct cause-effect mapping and indirect mapping through an intermediate conditioning variable. However, PCM remains limited to univariate delay embeddings in its cross-mapping processes. In this work, we extend PCM to the multivariate setting, introducing multiPCM, which leverages multivariate embeddings to more effectively distinguish indirect causal relationships. We further propose a multivariate cross-mapping framework (MXMap) for causal discovery in dynamical systems. This two-phase framework combines (1) pairwise CCM tests to establish an initial causal graph and (2) multiPCM to refine the graph by pruning indirect causal connections. Through experiments on simulated data and the ERA5 Reanalysis weather dataset, we demonstrate the effectiveness of MXMap. Additionally, MXMap is compared against several baseline methods, showing advantages in accuracy and causal graph refinement.}
}

@InProceedings{pmlr-v275-jamshidi25a,
	title = {Sample Complexity of Nonparametric Closeness Testing for Continuous Distributions and Its Application to Causal Discovery with Hidden Confounding},
	author = {Jamshidi, Fateme and Akbari, Sina and Kiyavash, Negar},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1217--1238},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {Dz31Rlv7S8},
	abstract = {We study the problem of closeness testing for continuous distributions and its implications for causal discovery. Specifically, we analyze the sample complexity of distinguishing whether two multidimensional continuous distributions are identical or differ by at least $\epsilon$ in terms of Kullback-Leibler (KL) divergence under non-parametric assumptions. To this end, we propose an estimator of KL divergence which is based on the von Mises expansion. Our closeness test attains optimal parametric rates under smoothness assumptions. Equipped with this test, which serves as a building block of our causal discovery algorithm to identify the causal structure between two multidimensional random variables, we establish sample complexity guarantees for our causal discovery method. To the best of our knowledge, this work is the first work that provides sample complexity guarantees for distinguishing cause and effect in multidimensional non-linear models with non-Gaussian continuous variables in the presence of unobserved confounding.}
}

@InProceedings{pmlr-v275-padh25a,
	title = {Your Assumed DAG is Wrong And Here's How To Deal With It},
	author = {Padh, Kirtan and Li, Zhufeng and Casolo, Cecilia and Kilbertus, Niki},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1239--1267},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {s2xxMeZJ4A},
	abstract = {Assuming a directed acyclic graph (DAG) that represents prior knowledge of causal relationships between variables is a common starting point for cause-effect estimation. Existing literature typically invokes hypothetical domain expert knowledge or causal discovery algorithms to justify this assumption. In practice, neither may propose a single DAG with high confidence: domain experts are hesitant to rule out dependencies with certainty or have ongoing disputes about relationships, whereas causal discovery often only provides an equivalence class of DAGs, relies on untestable assumptions itself, or are sensitive to hyperparameter and threshold choices. We propose an efficient, gradient-based optimization method that provides bounds for causal queries over a collection of plausible causal graphs given prior knowledge that may still be too large for exhaustive enumeration. We demonstrate excellent coverage and sharpness of our bounds for causal queries such as average treatment effect estimation in linear and non-linear synthetic settings as well as on real-world data. Our approach is an easy-to-use and widely applicable rebuttal to the valid critique of `What if your assumed DAG is wrong?'.}
}

@InProceedings{pmlr-v275-jalaldoust25a,
	title = {Multi-Domain Causal Discovery in Bijective Causal Models},
	author = {Jalaldoust, Kasra and Salehkaleybar, Saber and Kiyavash, Negar},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1268--1289},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {Li07fCvEhw},
	abstract = {We consider the problem of causal discovery (a.k.a., causal structure learning) in a multi-domain setting. We assume that the causal functions are invariant across the domains, while the distribution of the exogenous noise may vary. Under causal sufficiency (i.e., no confounders exist), we show that the causal diagram can be discovered under less restrictive functional assumptions compared to previous work. What enables causal discovery in this setting is bijective generation mechanisms (BGM), which ensures that the functional relation between the exogenous noise E and the endogenous variable Y is bijective and differentiable in both directions at every level of the cause variable X = x. BGM generalizes a variety of models including additive noise model, LiNGAM, post-nonlinear model, location-scale noise model. Further, we derive a statistical test to find the parents set of the target variable. Experiments on various synthetic and real-world datasets validate our theoretical findings.}
}

@InProceedings{pmlr-v275-lembo25a,
	title = {Causal drivers of dynamic networks},
	author = {Lembo, Melania and Riccardi, Ester and Vinciotti, Veronica and Wit, Ernst C.},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1290--1290},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {DNVWJCMN8q},
	abstract = {Dynamic networks models describe temporal interactions between social actors, and as such have been used to describe financial fraudulent transactions, dispersion of destructive invasive species across the globe, and the spread of fake news. An important question in all of these examples is what are the causal drivers underlying these processes. Current network models are exclusively descriptive and based on correlative structures.

In this paper we propose a causal extension of dynamic network modelling. In particular, we prove that the causal model satisfies a set of population conditions that uniquely identifies the causal drivers. The empirical analogue of these conditions provide a consistent causal discovery algorithm, which distinguishes it from other inferential approaches. Crucially, data from a single environment is sufficient. We apply the method in an analysis of bike sharing data in Washington D.C. in July 2023.}
}

@InProceedings{pmlr-v275-chatzi25a,
	title = {Counterfactual Token Generation in Large Language Models},
	author = {Chatzi, Ivi and Corvelo Benz, Nina L. and Straitouri, Eleni and Tsirtsis, Stratis and Gomez-Rodriguez, Manuel},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1291--1315},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {YYztiQ2Vke},
	abstract = {*"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...] Lyra's eyes welled up with tears as she realized the bitter truth - she had sacrificed everything for fleeting riches, and lost the love of her crew, her family, and herself."* Although this story, generated by a large language model, is captivating, one may wonder---how would the story have unfolded if the model had chosen "Captain Maeve" as the protagonist instead? We cannot know. State-of-the-art large language models are stateless---they maintain no internal memory or state. Given a prompt, they generate a sequence of tokens as an output using an autoregressive process. As a consequence, they cannot reason about counterfactual alternatives to tokens they have generated in the past. In this work, our goal is to enhance them with this functionality. To this end, we develop a causal model of token generation that builds upon the Gumbel-Max structural causal model. Our model allows any large language model to perform counterfactual token generation at almost no cost in comparison with vanilla token generation, it is embarrassingly simple to implement, and it does not require any fine-tuning nor prompt engineering. We implement our model on Llama 3 8B-Instruct and Ministral-8B-Instruct, and conduct a qualitative and a quantitative analysis of counterfactually generated text. We conclude with a demonstrative application of counterfactual token generation for bias detection, unveiling interesting insights about the model of the world constructed by large language models.}
}

@InProceedings{pmlr-v275-han25a,
	title = {Selecting Accurate Subgraphical Models from Possibly Inaccurate Graphical Models},
	author = {Han, Yi and Ramsey, Joseph and Spirtes, Peter},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1316--1346},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {c0qBiyiZ1L},
	abstract = {Methods of statistically testing the accuracy of causal graphical models have traditionally been
limited, with most focusing on parametric global assessments of the entire causal graph. However,
whether or not a causal graphical model passes a statistical test, it is crucial for many practical
applications to find which parts of the graph are accurately reconstructed and which are not. In this
paper, we introduce the Vertex Checker, the only statistical test that we are aware of that takes as
input a causal graphical model G, a vertex X, and an alpha level, sample data, and a conditional
independence test, and provides a non-parametric, asymptotically correct, statistical test of a local
subgraph of X, is computationally feasible for dozens of variables, and is extendable to other
kinds of causal graphical models. Through extensive simulations, we demonstrate the robustness
of the Vertex Checker across various data types, causal graphs, and distributions both in terms of
accuracy of graphical structure and of quantitative estimates of causal effects. Furthermore, we
apply the Vertex Checker to the real-world Sachs dataset, showcasing its practical applicability
in uncovering accurate substructures within causal graphs, even when the overall causal graphical
model is rejected.}
}

@InProceedings{pmlr-v275-barnard25a,
	title = {Temporal Inverse Probability Weighting for Causal Discovery in Controlled Before–After Studies: Discovering ADEs in Generics},
	author = {Barnard, Aubrey and Peissig, Peggy L. and Page, David},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1347--1364},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {xCtYBAVPZD},
	abstract = {Adverse drug events (ADEs) cost society lives and an estimated $30 billion per year in the USA alone.  Their prevalence has led to the public losing trust in the safety of drugs, especially generics (e.g., Eban, 2019).  These concerns have motivated the wide study of methods for general ADE discovery, but discovering ADEs in generic drugs challenges causal discovery methods with a scenario of multiple treatments over time, a scenario which presents new problems and opportunities for machine learning.  In response, this research develops methods for causal discovery based on analyzing controlled before\textendash After studies with differential prediction and temporal inverse probability weighting.  These methods are easy to realize by employing off-the-shelf machine learning classifiers.  Experiments on both synthetic and real electronic health records demonstrate the ability of the methods to control for confounding, discover generic-specific ADEs in synthetic data, and hypothesize brand\textendash generic differences in real-world data that agree with known ones.  These are the abilities that causal discovery methods need to help establish the facts of generic drug safety.}
}

@InProceedings{pmlr-v275-pruthi25a,
	title = {Compositional Models for Estimating Causal Effects},
	author = {Pruthi, Purva and Jensen, David},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1365--1404},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {yYgm7F4dNY},
	abstract = {Many real-world systems can be usefully represented as sets of interacting components. Examples include computational systems, such as query processors and compilers; natural systems, such as cells and ecosystems; and social systems, such as families and organizations. However, current approaches to estimating *potential outcomes* and *causal effects* typically treat such systems as single units, represent them with a fixed set of variables, and assume a homogeneous data-generating process. In this work, we study a *compositional* approach for estimating individual-level potential outcomes and causal effects in structured systems, where each unit is represented by an *instance-specific* composition of multiple heterogeneous components. The compositional approach decomposes unit-level causal queries into more fine-grained queries, explicitly modeling how unit-level interventions affect component-level outcomes to generate a unit's outcome. We demonstrate this approach using modular neural network architectures and show that it provides benefits for causal effect estimation from observational data, such as accurate causal effect estimation for structured units, increased sample efficiency, improved overlap between treatment and control groups, and compositional generalization to units with unseen combinations of components. Remarkably, our results show that compositional modeling can improve the accuracy of causal estimation even when component-level outcomes are unobserved. We also create and use a set of real-world evaluation environments for the empirical evaluation of compositional approaches for causal effect estimation and demonstrate the role of composition structure, varying amounts of component-level data access, and component heterogeneity in the performance of compositional models as compared to the non-compositional approaches.}
}

@InProceedings{pmlr-v275-saha25a,
	title = {On Measuring Intrinsic Causal Attributions in Deep Neural Networks},
	author = {Saha, Saptarshi and Rathore, Dhruv Vansraj and Saha, Soumadeep and Doermann, David and Garain, Utpal},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1405--1434},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {mCVuDRv1Eh},
	abstract = {Quantifying the causal influence of input features within neural networks has become a topic of increasing interest. Existing approaches typically assess direct, indirect, and total causal effects. This work treats NNs as structural causal models (SCMs) and extends our focus to include intrinsic causal contributions (ICC). We propose an identifiable generative post-hoc framework for quantifying ICC. We also draw a relationship between ICC and Sobol' indices. Our experiments on synthetic and real-world datasets demonstrate that ICC generates more intuitive and faithful explanations compared to existing global explanation techniques.}
}

@InProceedings{pmlr-v275-jahn25a,
	title = {Causal Identification in Time Series Models},
	author = {Jahn, Erik L and Karnik, Karthik and Schulman, Leonard},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1435--1449},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {0KrMZ7C6ov},
	abstract = {In this paper, we analyze the applicability of the Causal Identification algorithm to causal time series graphs with latent confounders. Since these graphs extend over infinitely many time steps, deciding whether causal effects across arbitrary time intervals are identifiable appears to require computation on graph segments of unbounded size. Even for deciding the identifiability of intervention effects on variables that are close in time, no bound is known on how many time steps in the past need to be considered. We give a first bound of this kind that only depends on the number of variables per time step and the maximum time lag of any direct or latent causal effect. More generally, we show that applying the Causal Identification algorithm to a constant-size segment of the time series graph is sufficient to decide identifiability of causal effects, even across unbounded time intervals.}
}

@InProceedings{pmlr-v275-ugadiarov25a,
	title = {Relational Object-Centric Actor-Critic},
	author = {Ugadiarov, Leonid Anatolievich and Vorobyov, Vitaliy and Panov, Aleksandr},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1450--1476},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {p2jpr9Na2G},
	abstract = {There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches, by incorporating an object-centric world model in critic. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. We evaluated our algorithm in simulated 3D robotic environment and a 2D environment with compositional structure. As baselines, we consider the state-of-the-art model-free actor-critic algorithm built upon transformer architecture and the state-of-the-art monolithic model-based algorithm. While the proposed method demonstrates comparable performance to the baselines in easier tasks, it outperforms the baselines within the 1M environment step budget in more challenging tasks increased number of objects or more complex dynamics.}
}

@InProceedings{pmlr-v275-howard25a,
	title = {Extending Structural Causal Models for Autonomous Vehicles to Simplify Temporal System Construction & Enable Dynamic Interactions Between Agents},
	author = {Howard, Rhys Peter Matthew and Kunze, Lars},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1477--1505},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {1CI5I7JBaU},
	abstract = {In this work we aim to bridge the divide between autonomous vehicles and causal reasoning. Autonomous vehicles have come to increasingly interact with human drivers, and in many cases may pose risks to the physical or mental well-being of those they interact with. Meanwhile causal models, despite their inherent transparency and ability to offer contrastive explanations, have found limited usage within such systems. As such, we first identify the challenges that have limited the integration of structural causal models within autonomous vehicles. We then introduce a number of theoretical extensions to the structural causal model formalism in order to tackle these challenges. This augments these models to possess greater levels of modularisation and encapsulation, as well presenting temporal causal model representation with constant space complexity. We also prove through the extensions we have introduced that dynamically mutable sets (e.g. varying numbers of autonomous vehicles across time) can be used within a structural causal model while maintaining a relaxed form of causal stationarity. Finally we discuss the application of the extensions in the context of the autonomous vehicle and service robotics domain along with potential directions for future work.}
}

@InProceedings{pmlr-v275-herman25a,
	title = {Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark Datasets for Causal Discovery},
	author = {Herman, Rebecca J. and Wahl, Jonas and Ninad, Urmi and Runge, Jakob},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1506--1531},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {vSlGli1EAy},
	abstract = {Causal discovery aims to extract qualitative causal knowledge in the form of causal graphs from data. Because causal ground truth is rarely known in the real world, simulated data plays a vital role in evaluating the performance of the various causal discovery algorithms proposed in the literature. But recent work highlighted certain artifacts of commonly used data generation techniques for a standard class of structural causal models (SCM) that may be nonphysical, including var- and R2-sortability, where the variables' variance and coefficients of determination (R2) after regressing on all other variables, respectively, increase along the causal order. Some causal methods exploit such artifacts, leading to unrealistic expectations for their performance on real-world data. Some modifications have been proposed to remove these artifacts; notably, the internally-standardized structural causal model (iSCM) avoids varsortability and largely alleviates R2-sortability on sparse causal graphs, but exhibits a reversed R2-sortability pattern for denser graphs not featured in their work. We analyze which sortability patterns we expect to see in real data, and propose a method for drawing coefficients that we argue more effectively samples the space of SCMs. Finally, we propose a novel extension of our SCM generation method to the time series setting.}
}

@InProceedings{pmlr-v275-beckers25b,
	title = {Nondeterministic Causal Models},
	author = {Beckers, Sander},
	booktitle = {Proceedings of the Fourth Conference on Causal Learning and Reasoning},
	pages = {1532--1554},
	year = {2025},
	editor = {Huang, Biwei and Drton, Mathias},
	volume = {275},
	series = {Proceedings of Machine Learning Research},
	month = {may},
	publisher = {PMLR},
	openreview = {F5D1Xo5d74},
	abstract = {I generalize acyclic deterministic structural causal models to the nondeterministic case and argue that this offers an improved semantics for counterfactuals. The standard, deterministic, semantics developed by Halpern (and based on the initial proposal of Galles & Pearl) assumes that for each assignment of values to parent variables there is a unique assignment to their child variable, and it assumes that the actual world (an assignment of values to all variables of a model) specifies a unique counterfactual world for each intervention. Both assumptions are unrealistic, and therefore I drop both of them in my proposal. I do so by allowing multi-valued functions in the structural equations. In addition, I adjust the semantics so that the solutions to the equations that obtained in the actual world are preserved in any counterfactual world. I provide a sound and complete axiomatization of the resulting logic and compare it to the standard one by Halpern and to more recent proposals that are closer to mine. Finally, I extend these models to the probabilistic case and show that they open up the way to identifying counterfactuals even in Causal Bayesian Networks.}
}

